import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# âœ… íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ê²½ë¡œ
fine_tuned_model_path = "./llama"

# âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ (íŒ¨ë”© í† í° ì„¤ì •)
tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)
tokenizer.pad_token_id = tokenizer.eos_token_id  # ğŸš€ íŒ¨ë”© ë¬¸ì œ í•´ê²°

# âœ… ëª¨ë¸ ë¡œë“œ (VRAM ì ˆì•½ì„ ìœ„í•œ float16 ì ìš©)
device = "cuda" if torch.cuda.is_available() else "cpu"

model = AutoModelForCausalLM.from_pretrained(
    fine_tuned_model_path,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32,  # âœ… VRAM ì ˆì•½
    low_cpu_mem_usage=True  # âœ… ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì í™”
).to(device)

# âœ… ëª¨ë¸ FP16 ë˜ëŠ” BF16 ì ìš© (VRAM ë¶€ì¡± ë¬¸ì œ ë°©ì§€)
if device == "cuda":
    model = model.half()  # âœ… VRAM ì ˆì•½

print("ğŸ‰ LLaMA2 ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ ì‹œì‘! 'exit'ì„ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.")

# ğŸ”¹ ëŒ€í™” ë£¨í”„ (ë¬´í•œ ë°˜ë³µ)
while True:
    user_input = input("\nğŸ‘¤ [ì‚¬ìš©ì]: ")

    if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
        print("ğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤!")
        break  # ë£¨í”„ ì¢…ë£Œ

    # ğŸ”¹ ì…ë ¥ì„ í† í¬ë‚˜ì´ì§• (íŒ¨ë”© & ê¸¸ì´ ì„¤ì • ì¶”ê°€)
    input_ids = tokenizer(
        user_input,
        return_tensors="pt",
        padding=True,  # âœ… max_length ì—†ì´ ìë™ íŒ¨ë”©
        truncation=True,
        max_length=512  # ğŸš€ í† í° ê¸¸ì´ ì„¤ì •
    ).input_ids.to(device)

    # ğŸ”¹ ëª¨ë¸ ì˜ˆì¸¡ ìƒì„± (ìµœì í™”ëœ ì„¤ì • ì ìš©)
    output = model.generate(
        input_ids,
        max_new_tokens=100,  # âœ… ì¶”ê°€ ìƒì„± í† í° ê°œìˆ˜ ì œí•œ
        temperature=0.7,  # ì°½ì˜ì ì¸ ë‹µë³€ì„ ì›í•˜ë©´ ë†’ì´ê³ , ë³´ìˆ˜ì ì´ë©´ ë‚®ì¶¤
        top_k=50,         # ìƒìœ„ Kê°œ ë‹¨ì–´ ì¤‘ ì„ íƒ
        top_p=0.9,        # í™•ë¥  ë†’ì€ ë‹¨ì–´ ìœ„ì£¼ ì„ íƒ
        repetition_penalty=1.2,  # ğŸš€ ë°˜ë³µë˜ëŠ” ë‹¨ì–´ ë°©ì§€
        eos_token_id=tokenizer.eos_token_id  # ğŸš€ ì˜ëª»ëœ ë°˜ë³µ ìƒì„± ë°©ì§€
    )

    # ğŸ”¹ ê²°ê³¼ ì¶œë ¥ (í”„ë¡¬í”„íŠ¸ ì œì™¸)
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    
    # âœ… í”„ë¡¬í”„íŠ¸ ì œê±°
    if user_input in response:
        response = response.split(user_input)[-1].strip()

    print(f"\nğŸ¤– [AI]: {response}")

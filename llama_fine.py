from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq
from peft import LoraConfig, get_peft_model
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer

# ğŸ”¹ LLaMA2 ëª¨ë¸ ë¡œë“œ
model_name = "Bllossom/llama-3.2-Korean-Bllossom-3B"
tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)

# ğŸ”¹ íŒ¨ë”© í† í° ì„¤ì • (ì˜¤ë¥˜ í•´ê²° âœ…)
tokenizer.pad_token = tokenizer.eos_token

# ğŸ”¹ LoRA ì„¤ì • (ë¹ ë¥¸ íŒŒì¸íŠœë‹ì„ ìœ„í•´ ì ìš©)
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    target_modules=["q_proj", "v_proj"]
)

# ğŸ”¹ ëª¨ë¸ì— LoRA ì ìš©
model = AutoModelForCausalLM.from_pretrained(model_name, token=True)
model.resize_token_embeddings(len(tokenizer))  # ìƒˆë¡œìš´ pad_tokenì„ ë°˜ì˜í•  ê²½ìš° í•„ìˆ˜

model = get_peft_model(model, lora_config)

# ğŸ”¹ JSONL ë°ì´í„°ì…‹ ë¡œë“œ
dataset = load_dataset("json", data_files="train_data.jsonl")


# ğŸ”¹ ë°ì´í„° ë³€í™˜ í•¨ìˆ˜
def preprocess_function(example):
    user_prompt = ""
    assistant_response = ""

    # ğŸ”¹ messages ë¦¬ìŠ¤íŠ¸ì—ì„œ userì™€ assistant ì—­í•  ì°¾ê¸°
    for message in example["messages"]:
        if message["role"] == "user":
            user_prompt = message["content"]
        elif message["role"] == "assistant":
            assistant_response = message["content"]

    # ğŸ”¹ ì˜¬ë°”ë¥¸ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    text = f"User: {user_prompt}\nAI: {assistant_response}"

    # ğŸ”¹ í† í¬ë‚˜ì´ì§• í›„ ë°˜í™˜ (íŒ¨ë”© ì„¤ì • ì¶”ê°€ âœ…)
    tokenized = tokenizer(text, padding=True, truncation=True, max_length=512)

    # ğŸ”¹ labelsë¥¼ íŒ¨ë”© ë¶€ë¶„ë§Œ -100ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ loss ê³„ì‚° ì˜¤ë¥˜ ë°©ì§€
    labels = tokenized["input_ids"]
    labels = [-100 if token == tokenizer.pad_token_id else token for token in labels]
    return {
        "input_ids": tokenized["input_ids"],
        "attention_mask": tokenized["attention_mask"],
        "labels": labels  # âœ… íŒ¨ë”© í† í°ì„ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
    }

# ğŸ”¹ `batched=False`ë¥¼ ìœ ì§€í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ ì¤‘ì²© ë°©ì§€
tokenized_dataset = dataset.map(preprocess_function, batched=False, remove_columns=["messages"])

# ğŸ”¹ ë°ì´í„° í¬ê¸° ì„¤ì •
num_examples = 40000  # ë°ì´í„° ê°œìˆ˜
batch_size = 2  # ë°°ì¹˜ í¬ê¸° (ì‘ì€ GPUë©´ 1ë¡œ ì„¤ì • ê°€ëŠ¥)
gradient_accumulation_steps = 4  # ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¥¼ ë³´ì™„í•˜ëŠ” ëˆ„ì  ê·¸ë˜ë””ì–¸íŠ¸
max_steps = int(num_examples / batch_size) * 2  # ë°ì´í„°ì…‹ì„ 2ë²ˆ ë°˜ë³µ í•™ìŠµ

print(f"ğŸ”¥ ìµœì í™”ëœ max_steps: {max_steps}")  # ì˜ˆìƒê°’ í™•ì¸

# ğŸ”¹ í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./llama_finetuned",
    per_device_train_batch_size=batch_size,  
    gradient_accumulation_steps=gradient_accumulation_steps,
    warmup_steps=100,
    max_steps=max_steps, # ì „ì²´ ë°ì´í„°ì…‹ì„ 2ë²ˆ ë°˜ë³µ í•™ìŠµ
    # learning_rate=2e-5,
    fp16=True,  # 16-bit í•™ìŠµ (ë©”ëª¨ë¦¬ ì ˆì•½)
    logging_dir="./logs",
    save_steps=2000,  # 2000 ìŠ¤í…ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    save_total_limit=3,
    remove_unused_columns=False
)
print("\nğŸ“‚ [ë””ë²„ê¹…] Trainerì— ì „ë‹¬í•  ìµœì¢… ë°ì´í„°ì…‹ ì²« 3ê°œ ìƒ˜í”Œ:")
print(tokenized_dataset["train"][:3]) 
print("\nğŸ“‚ [ë””ë²„ê¹…] tokenized_datasetì˜ í‚¤ë“¤:", tokenized_dataset.keys())
# ğŸ”¹ ë°ì´í„° Collator ì„¤ì •
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
if "train" in tokenized_dataset:
    final_dataset = tokenized_dataset["train"]
else:
    final_dataset = tokenized_dataset
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],  
    data_collator=data_collator
)

# ğŸ”¹ íŒŒì¸íŠœë‹ ì‹¤í–‰ ğŸš€
trainer.train()

# ğŸ”¹ ëª¨ë¸ ì €ì¥
model.save_pretrained("./llama")
tokenizer.save_pretrained("./llama")
print("ğŸ‰ LLaMA2 íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ì´ ./llama_finetuned ì— ì €ì¥ë¨.")

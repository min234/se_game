from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model
from datasets import load_dataset
import torch

# âœ… ëª¨ë¸ ì´ë¦„
model_name = "meta-llama/Llama-3.1-8B-Instruct"

# âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  # padding ì‹œ EOSë¡œ ì±„ì›€

# âœ… ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",  # GPU ìë™ í• ë‹¹
    torch_dtype=torch.bfloat16,  # ë˜ëŠ” float16 (Windowsì—ì„œ)
    load_in_4bit=True
)

# âœ… LoRA êµ¬ì„±
peft_config = LoraConfig(
    r=32,
    lora_alpha=64,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

# âœ… LoRA ì ìš©
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, peft_config)

# âœ… ë°ì´í„° ë¡œë“œ
dataset = load_dataset("json", data_files="memo.jsonl", split="train")

# âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±
prompt_template = """### Instruction: {}\n### Question: {}\n### Answer: {}"""

# âœ… ìµœëŒ€ ê¸¸ì´ ìë™ ê³„ì‚° (ë°ì´í„° ê¸°ë°˜ max_length ì„¤ì •)
def build_prompt(example):
    return prompt_template.format(example["instruction"], example["input"], example["output"])

token_lens = [len(tokenizer(build_prompt(e))["input_ids"]) for e in dataset]
max_length = max(token_lens)
print(f"âœ… ìµœëŒ€ í† í° ê¸¸ì´ (max_length): {max_length}")

# âœ… ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
def format(example):
    prompt = build_prompt(example)
    tokenized = tokenizer(prompt, truncation=True, padding="max_length", max_length=max_length, return_tensors="pt")
    return {
        "input_ids": tokenized["input_ids"][0],
        "attention_mask": tokenized["attention_mask"][0],
        "labels": tokenized["input_ids"][0]
    }

# âœ… train / eval ë‚˜ëˆ„ê¸° (90:10)
split_dataset = dataset.train_test_split(test_size=0.1)
train_dataset = split_dataset["train"].map(format, batched=False, remove_columns=["lang", "vulnerability", "system","question","chosen","rejected"])
eval_dataset  = split_dataset["test"].map(format, batched=False, remove_columns=["lang", "vulnerability", "system","question","chosen","rejected"])
from transformers import TrainerCallback

class CustomLoggerCallback(TrainerCallback):
    def on_step_end(self, args, state, control, **kwargs):
        logs = kwargs.get("logs", {})
        loss = logs.get("loss", None)

        if loss is not None:
            print(f"ğŸ”„ Step {state.global_step}/{state.max_steps} | Loss: {loss:.4f} | Epoch: {state.epoch:.2f}")
# âœ… í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./trans",
    per_device_train_batch_size=8,
    gradient_accumulation_steps=8,
    num_train_epochs=20,
    logging_dir="./logs",
    logging_steps=20,
    save_steps=500,
    save_total_limit=2,
    remove_unused_columns=False,
    # âœ… í‰ê°€ ë° ì €ì¥
    evaluation_strategy="steps",   # ë§¤ stepë§ˆë‹¤ í‰ê°€
    eval_steps=500,                # 500 stepë§ˆë‹¤ í‰ê°€
    load_best_model_at_end=True,   # ê°€ì¥ ë‚®ì€ loss ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ ì €ì¥
    metric_for_best_model="loss",
    greater_is_better=False,

    save_strategy="steps",         # stepë§ˆë‹¤ ì €ì¥

    # âœ… mixed precision
    fp16=True,
)

# âœ… Trainer ìƒì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,  # âœ… í‰ê°€ìš© ë°ì´í„° ê¼­ ë„£ê¸°
    callbacks=[CustomLoggerCallback()] 
)

# âœ… í•™ìŠµ ì‹œì‘
print("âœ… í•™ìŠµ ì‹œì‘")
trainer.train()

# âœ… ëª¨ë¸ ì €ì¥
trainer.model.save_pretrained("./transs")
tokenizer.save_pretrained("./transs")
print("âœ… ì €ì¥ ì™„ë£Œ")

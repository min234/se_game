from transformers import AutoTokenizer,AutoModelForCausalLM,TrainingArguments,Trainer
from datasets import load_dataset
from peft import LoraConfig, get_peft_model
import numpy as np
import torch 

model_name = "meta-llama/Llama-3.2-1B"

tokenizer = AutoTokenizer.from_pretrained(model_name)

if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token':'[PAD]'})

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    target_modules=["q_proj","k_proj","v_proj","o_proj",]
)
model = AutoModelForCausalLM.from_pretrained(model_name,device_map='auto', torch_dtype=torch.float16)

lora_model = get_peft_model(model,lora_config)

dataset = load_dataset("json", data_files="isekai_yui_dataset.jsonl")

# ğŸ”¹ 'train' ìŠ¤í”Œë¦¿ ì„ íƒ
train_dataset = dataset["train"]

# ğŸ”¹ ìµœëŒ€ í† í° ê¸¸ì´ ê³„ì‚° (í•œ ë²ˆë§Œ ì‹¤í–‰)
token_lens = [len(tokenizer.encode(sample["input"] + " " + sample["output"], add_special_tokens=True)) for sample in train_dataset]
max_token_length = int(np.percentile(token_lens, 95))  # ìƒìœ„ 95% ìƒ˜í”Œ ê¸°ì¤€
print(f"ğŸ“Œ ì„¤ì •ëœ max_length: {max_token_length}")

# ğŸ”¹ ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_function(example):
    user_input = example["input"]
    assistant_response = example["output"]

    # âœ… í•™ìŠµ ë°ì´í„° í…ìŠ¤íŠ¸ í¬ë§· ì„¤ì • (QA êµ¬ì¡°)
    text = f"ì‚¬ìš©ì: {user_input}\nìœ ì´: {assistant_response}"

    # âœ… í† í°í™” ë° íŒ¨ë”© ì ìš©
    tokenized = tokenizer(
        text,
        truncation=True,
        padding="max_length",  # âœ… ìµœëŒ€ ê¸¸ì´ì— ë§ì¶° íŒ¨ë”©
        max_length=max_token_length,
        return_tensors="pt"
    )

    return {
        "input_ids": tokenized["input_ids"][0].tolist(),
        "attention_mask": tokenized["attention_mask"][0].tolist(),
        "labels": tokenized["input_ids"][0].tolist()
    }

# ğŸ”¹ ë°ì´í„°ì…‹ ë³€í™˜ (KeyError ë°©ì§€)
tokenized_dataset = train_dataset.map(preprocess_function, batched=False, remove_columns=["input", "output"])

print("\nğŸ‰ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")

training_args = TrainingArguments(
    output_dir="./yui_finetuned",
    num_train_epochs=3,  # âœ… í•™ìŠµ íšŸìˆ˜ ì¦ê°€
    bf16=True,
    per_device_train_batch_size=8,  # âœ… LoRAëŠ” ê°€ë²¼ìš°ë¯€ë¡œ 8~16 ê°€ëŠ¥
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    learning_rate=2e-5,  # âœ… 2e-6ì€ ë„ˆë¬´ ì‘ìŒ â†’ 2e-5ë¡œ ë³€ê²½
    max_steps=2500,  # âœ… `max_stps` â†’ `max_steps` ìˆ˜ì •
    save_steps=500,
    logging_steps=20,
    save_total_limit=2,
)

trainer = Trainer(
    model=lora_model,  # âœ… LoRA ì ìš©ëœ ëª¨ë¸ ì‚¬ìš©!
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
)

trainer.train()

trainer.model.save_pretrained("./yui_finetuned")
tokenizer.save_pretrained("./yui_finetuned")

print("íŒŒì¸ íŠœë‹ ì™„ë£Œ ã…Šã…‹ã…Šã…‹")
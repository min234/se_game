from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ëª¨ë¸ ê²½ë¡œ
model_path = "./securitys"

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map="auto")

# Instruction í…œí”Œë¦¿
instruction_text = """ë‹¤ìŒ ì½”ë“œë¥¼ ë³´ê³  ì–¸ì–´, ì·¨ì•½ì  ì¢…ë¥˜, ìˆ˜ì •ëœ ì½”ë“œë¥¼ ìˆœì„œëŒ€ë¡œ ì¶œë ¥í•´ì¤˜.
ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ì§€ì¼œì¤˜:

ì–¸ì–´: (ì½”ë“œ ì–¸ì–´)
ì·¨ì•½ì : (ì·¨ì•½ì  ì„¤ëª…)
ìˆ˜ì •ëœ ì½”ë“œ:
(ì·¨ì•½ì ì´ ìˆ˜ì •ëœ ì½”ë“œ)"""

# í…ŒìŠ¤íŠ¸ ì½”ë“œ ì…ë ¥
test_code = """
import os

def run_command(cmd):
    os.system("ping " + cmd)

user_input = input("Enter hostname or IP: ")
run_command(user_input)
"""

# í”„ë¡¬í”„íŠ¸ ìƒì„±
prompt = f"""<|begin_of_text|>
### Instruction:
{instruction_text}

### Input:
{test_code}

### Response:
"""

# í† í¬ë‚˜ì´ì¦ˆ
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# ìƒì„±
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )

# ê²°ê³¼ ë””ì½”ë”©
decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
response_part = decoded.split("### Response:")[-1].strip()

print("\nğŸ“¤ [ëª¨ë¸ ì‘ë‹µ ê²°ê³¼]")
print(response_part)

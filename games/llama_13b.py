from transformers import AutoModelForCausalLM, LlamaTokenizer, TrainingArguments, Trainer, TrainerCallback
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model
from datasets import load_dataset
import torch
import numpy as np

# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "meta-llama/Llama-2-13b-hf"
# model_name = r"c:\hf\Llama-3.3-70B-Instruct"
#llama2 ë²„ì „ì€ llamatokenzier ì‚¬ìš©
tokenizer = LlamaTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
# âœ… pad_token ë³„ë„ ì¶”ê°€ ë° í† í¬ë‚˜ì´ì € í™•ì¥
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({
        "pad_token": "<|pad|>",
        "eos_token": "<|end_of_text|>",
        "bos_token": "<|begin_of_text|>",
        "unk_token": "<|unk|>",
    })

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    load_in_4bit=True
)

# âœ… ëª¨ë¸ ì„ë² ë”© í¬ê¸° ì¬ì¡°ì •
model.resize_token_embeddings(len(tokenizer))

# âœ… LoRA êµ¬ì„± ë° ì ìš©
peft_config = LoraConfig(
    r=32,
    lora_alpha=64,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj","gate_proj","up_proj","down_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, peft_config)

print(model)

# âœ… ë°ì´í„° ë¡œë“œ
dataset = load_dataset("CyberNative/Code_Vulnerability_Security_DPO")["train"]
print(dataset[0]["chosen"])

# âœ… Instruction í…œí”Œë¦¿
instruction_text = """ë‹¤ìŒ ì½”ë“œë¥¼ ë³´ê³  ì–¸ì–´, ì·¨ì•½ì  ì¢…ë¥˜, ìˆ˜ì •ëœ ì½”ë“œë¥¼ ìˆœì„œëŒ€ë¡œ ì¶œë ¥í•´ì¤˜.
ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ì§€ì¼œì¤˜:

ì–¸ì–´: (ì½”ë“œ ì–¸ì–´)
ì·¨ì•½ì : (ì·¨ì•½ì  ì„¤ëª…)
ìˆ˜ì •ëœ ì½”ë“œ:
(ì·¨ì•½ì ì´ ìˆ˜ì •ëœ ì½”ë“œ)"""

# âœ… í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
def build_prompt(example):
    code = example.get("rejected", example.get("input", "")).strip()
    prompt = f"""<|begin_of_text|>
### Instruction:
{instruction_text}

### Input:
{code}

### Response:
"""
    return prompt

# âœ… max_length ì¸¡ì •
print("ğŸ“ ìµœëŒ€ í† í° ê¸¸ì´ ì¸¡ì • ì¤‘...")
all_lengths = []
for ex in dataset:
    prompt = build_prompt(ex)
    if prompt and ex.get("chosen"):
        full = prompt + ex["chosen"] + tokenizer.eos_token
        all_lengths.append(len(tokenizer(full)["input_ids"]))

max_length = int(np.percentile(all_lengths, 95))  # ğŸ”¥ ìµœëŒ€ê°’ ëŒ€ì‹  95% ê¸¸ì´
print(f"âœ… í•™ìŠµì— ì‚¬ìš©í•  max_length: {max_length}")

# âœ… ì „ì²˜ë¦¬ í•¨ìˆ˜
def format(example):
    prompt = build_prompt(example)
    output = example.get("chosen", "").strip()
    
    # full í…ìŠ¤íŠ¸ì— eos_tokenì€ ì—¬ê¸°ì„œ ì œê±°
    full_text = prompt + output
    tokenized = tokenizer(
        full_text,
        truncation=True,
        padding="max_length",
        max_length=max_length
    )

    input_ids = tokenized["input_ids"]
    attention_mask = tokenized["attention_mask"]

    prompt_ids = tokenizer(prompt, truncation=True, max_length=max_length)["input_ids"]
    prompt_len = len(prompt_ids)

    labels = [-100] * prompt_len + input_ids[prompt_len:]

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

# âœ… ë°ì´í„° ë¶„ë¦¬ ë° ì „ì²˜ë¦¬
split_dataset = dataset.train_test_split(test_size=0.1)
train_dataset = split_dataset["train"].map(format, remove_columns=dataset.column_names)
eval_dataset = split_dataset["test"].map(format, remove_columns=dataset.column_names)
print(model)
print(f"âœ… train_dataset size: {len(train_dataset)}")
print(f"âœ… eval_dataset size: {len(eval_dataset)}")

# âœ… ìƒ˜í”Œ í™•ì¸
sample = train_dataset[0]
decoded_input = tokenizer.decode(sample["input_ids"], skip_special_tokens=False)
decoded_label = tokenizer.decode(
    [t for t in sample["labels"] if t != -100],
    skip_special_tokens=True
)

print("\nğŸ“¥ [ë””ì½”ë”©ëœ Input]:\n")
print(decoded_input)
print("\nğŸ“¤ [ë””ì½”ë”©ëœ Label]:\n")
print(decoded_label)

# âœ… ì»¤ìŠ¤í…€ ë¡œê±°
class CustomLoggerCallback(TrainerCallback):
    def on_step_end(self, args, state, control, **kwargs):
        loss = kwargs.get("logs", {}).get("loss", None)
        if loss is not None:
            print(f"ğŸ”„ Step {state.global_step}/{state.max_steps} | Loss: {loss:.4f} | Epoch: {state.epoch:.2f}")

# âœ… í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./secu_13b",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=5,
    logging_dir="./logs",
    logging_steps=20,
    save_steps=500,
    save_total_limit=2,
    evaluation_strategy="steps",
    eval_steps=500,
    load_best_model_at_end=True,
    metric_for_best_model="loss",
    greater_is_better=False,
    save_strategy="steps",
    bf16=True,
    gradient_checkpointing=True
)

# âœ… Trainer ì„¤ì •
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    callbacks=[CustomLoggerCallback()]
)

# âœ… í•™ìŠµ ì‹œì‘
print("âœ… í•™ìŠµ ì‹œì‘")
trainer.train()

# âœ… ëª¨ë¸ ì €ì¥
trainer.model.save_pretrained("./securitys_13b")
tokenizer.save_pretrained("./securitys_13b")
print("âœ… ì €ì¥ ì™„ë£Œ")
